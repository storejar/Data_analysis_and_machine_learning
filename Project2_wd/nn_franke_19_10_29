#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Oct 25 18:08:45 2019

@author: Ary
"""


import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score


from sklearn.metrics import accuracy_score


np.random.seed(2204)

## part a

def FrankeFunction(x,y):
    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))
    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))
    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))
    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)
    return term1 + term2 + term3 + term4

def Design_Matrix_X(x, y, n):
	N = len(x)
	l = int((n+1)*(n+2)/2)		
	X = np.ones((N,l))

	for i in range(1,n+1):
		q = int((i)*(i+1)/2)
		for k in range(i+1):
			X[:,q+k] = x**(i-k) * y**k

	return X

n_x=1000
m=5

x = np.random.uniform(0, 1, n_x)
y = np.random.uniform(0, 1, n_x)

z = FrankeFunction(x, y)

n = int(len(x))
z_1 = z +0.01*np.random.randn(n)
z_1, z_1=np.meshgrid(z_1, z_1)
#print(z_1.shape)
X= Design_Matrix_X(x,y,n=m)
DesignMatrix = pd.DataFrame(X)

trainingShare = 0.8
seed  = 1
XTrain, XTest, yTrain, yTest=train_test_split(X, z_1, train_size=trainingShare, \
                                              test_size = 1-trainingShare,
                                             random_state=seed)

n_inputs, n_features = XTrain.shape
#come scelgo il numero di categorie
n_categories = 1000
n_hidden_neurons1 = 50
n_hidden_neurons2 = 40
n_hidden_neurons3 = 30

def relu(X):
   return np.maximum(0,X)


# weights and bias in the hidden layer
hidden_weights1 = np.random.randn(n_features, n_hidden_neurons1)
hidden_bias1 = np.zeros(n_hidden_neurons1) + 0.01

hidden_weights2 = np.random.randn(n_hidden_neurons1, n_hidden_neurons2)
hidden_bias2 = np.zeros(n_hidden_neurons2) + 0.01

hidden_weights3 = np.random.randn(n_hidden_neurons2, n_hidden_neurons3)
hidden_bias3 = np.zeros(n_hidden_neurons3) + 0.01

# weights and bias in the output layer
output_weights = np.random.randn(n_hidden_neurons3, n_categories)
output_bias = np.zeros(n_categories) + 0.01

print(hidden_weights1)
print(hidden_weights2)
print(hidden_weights3)

def feed_forward(X):
    # weighted sum of inputs to the hidden layer
    z_h1 = np.matmul(X, hidden_weights1) + hidden_bias1
    #print(z_h1)
    # activation in the hidden layer
    a_h1 = relu(z_h1)
    #print(a_h1)
    
    z_h2 = np.matmul(a_h1, hidden_weights2) + hidden_bias2
    #print(z_h2)
    # activation in the hidden layer
    a_h2 = relu(z_h2)
    #print(a_h2)
    
    z_h3 = np.matmul(a_h2, hidden_weights3) + hidden_bias3
    #print(z_h3)
    # activation in the hidden layer
    a_h3 = relu(z_h3)
    #print(a_h3)
    
    # weighted sum of inputs to the output layer
    z_o = np.matmul(a_h3, output_weights) + output_bias
    #print(z_o) 
    
    outputs = relu(z_o) #is it the right function for output values?
    #print(outputs) 
    
    return outputs

probabilities = feed_forward(XTrain)
# =============================================================================
# print("probabilities = (n_inputs, n_categories) = " + str(probabilities.shape))
# print("probability that image 0 is in category 0,1,2,...,9 = \n" + str(probabilities[0]))
# print("probabilities sum up to: " + str(probabilities[0].sum()))
# print()
# 
# =============================================================================
# we obtain a prediction by taking the class with the highest likelihood
def predict(X):
    probabilities = feed_forward(X)
    return np.argmax(probabilities, axis=1)

# =============================================================================
# predictions = predict(XTrain)
# print("predictions = (n_inputs) = " + str(predictions.shape))
# print("prediction for image 0: " + str(predictions[0]))
# print("correct label for image 0: " + str(yTrain[0]))
# =============================================================================

def feed_forward_train(X):
# weighted sum of inputs to the hidden layer
    z_h1 = np.matmul(X, hidden_weights1) + hidden_bias1
    #print(z_h1.shape)
    # activation in the hidden layer
    a_h1 = relu(z_h1)
    #print(a_h1.shape)
    
    z_h2 = np.matmul(a_h1, hidden_weights2) + hidden_bias2
    #print(z_h2.shape)
    # activation in the hidden layer
    a_h2 = relu(z_h2)
    #print(a_h2.shape)
    
    z_h3 = np.matmul(a_h2, hidden_weights3) + hidden_bias3
    #print(z_h3.shape)
    # activation in the hidden layer
    a_h3 = relu(z_h3)
    
    # weighted sum of inputs to the output layer
    z_o = np.matmul(a_h3, output_weights) + output_bias
    # axis 0 holds each input and axis 1 the probabilities of each category
    #print(z_o.shape)
    outputs = relu(z_o)
    #print(outputs.shape)
    #print(outputs)
    
    print(a_h1)
    print(a_h2)
    print(a_h3)
    # for backpropagation need activations in hidden and output layers
    return a_h1, a_h2, a_h3, outputs

def backpropagation(X, Y):
    a_h1, a_h2, a_h3, outputs = feed_forward_train(X)
  
    # error in the output layer
    error_output = outputs - Y
    
    #print(outputs)
    #print(Y)
   # print(error_output)
    # error in the hidden layer
    if np.count_nonzero(a_h3) != 0:
        error_hidden3 = np.matmul(error_output, output_weights.T)
    else:
        error_hidden3 = np.zeros((error_output.shape[0],n_hidden_neurons3))
   # print(error_hidden3)
      
    # gradients for the output layer
    output_weights_gradient = np.matmul(a_h3.T, error_output)
    output_bias_gradient = np.sum(error_output, axis=0)
    #print(output_weights_gradient)
    #print(output_bias_gradient)
    
    # gradient for the hidden layer
    hidden_weights_gradient3 = np.matmul(a_h2.T, error_hidden3)
    hidden_bias_gradient3 = np.sum(error_hidden3, axis=0)
    
    if np.count_nonzero(a_h2) != 0:
        error_hidden2 = np.matmul(error_hidden3, hidden_weights3.T)
    else:
        error_hidden2 = np.zeros((error_hidden3.shape[0],n_hidden_neurons2))
        
    hidden_weights_gradient2 = np.matmul(a_h1.T, error_hidden2)
    hidden_bias_gradient2 = np.sum(error_hidden2, axis=0)
    
    if np.count_nonzero(a_h1) != 0:
        error_hidden1 = np.matmul(error_hidden2, hidden_weights2.T)
    else:
        error_hidden1 = np.zeros((error_hidden2.shape[0],n_hidden_neurons1))
        
    hidden_weights_gradient1 = np.matmul(X.T, error_hidden1)
    hidden_bias_gradient1 = np.sum(error_hidden1, axis=0)
    
    return output_weights_gradient, output_bias_gradient, hidden_weights_gradient1, hidden_bias_gradient1,hidden_weights_gradient2, hidden_bias_gradient2,hidden_weights_gradient3, hidden_bias_gradient3


#print("Old accuracy on training data: " + str(accuracy_score(predict(XTrain), yTrain)))

eta = 0.0001
lmbd = 0.0001
for i in range(3):
    # calculate gradients
    dWo, dBo, dWh1, dBh1, dWh2, dBh2, dWh3, dBh3 = backpropagation(XTrain, yTrain)
    #since I get really high values, I am not able to have a finite value in weights,
    #therefore I get nan as result
    
# =============================================================================
    print(dWo)
    print(dWh1)
    print(dWh2)
    print(dWh3)
#     
#     print ('---')
# =============================================================================
    
    # regularization term gradients
    dWo += lmbd * output_weights
    dWh1 += lmbd * hidden_weights1
    dWh2 += lmbd * hidden_weights2
    dWh3 += lmbd * hidden_weights3
    
# =============================================================================
#     print(dWo)
#     print(dWh1)
#     print(dWh2)
#     print(dWh3)
# =============================================================================
    
    
    # update weights and biases
    output_weights -= eta * dWo
    output_bias -= eta * dBo
    hidden_weights1 -= eta * dWh1
    hidden_bias1 -= eta * dBh1
    hidden_weights2 -= eta * dWh2
    hidden_bias2 -= eta * dBh2
    hidden_weights3 -= eta * dWh3
    hidden_bias3 -= eta * dBh3
    
    print(output_weights)
    print(hidden_weights1)
    print(hidden_weights2)
    print(hidden_weights3)
    
    

#print("New accuracy on training data: " + str(accuracy_score(predict(XTrain), yTrain)))
test_predict = predict(XTest)
print(test_predict)
y_values=yTest[:,0] 
print(y_values)

print("Mean squared error: %.10f" % mean_squared_error(y_values, test_predict))
print('Variance score: %.10f' % r2_score(y_values, test_predict))
#should be around 0.99 according to scikit-learn, it's -2.


# =============================================================================
# print(test_predict.shape)
# print(yTest.shape)
# =============================================================================
#print("Accuracy score on test set: ", accuracy_score(yTest, test_predict))



eta_vals = np.logspace(-5, 1, 7)
lmbd_vals = np.logspace(-5, 1, 7)

from sklearn.neural_network import MLPRegressor


for i, eta in enumerate(eta_vals):
    for j, lmbd in enumerate(lmbd_vals):
        dnn = MLPRegressor(hidden_layer_sizes=(n_hidden_neurons1, n_hidden_neurons2, n_hidden_neurons3),alpha=lmbd, learning_rate_init=eta, max_iter=100)
        dnn.fit(XTrain, yTrain[:,0])
        
        
        print("Learning rate  = ", eta)
        print("Lambda = ", lmbd)
        print("R2 on test set: ", dnn.score(XTest, yTest[:,0]))
        print()
        
# we need to find the right way to implement back propagation for linear regression, since the way it is produces high gradient numbers
# and leads to 0 predictions. Moreover, we need to check why some activations happen to be infinity.
