#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Oct  8 14:35:59 2019

@author: Ary
"""

import pandas as pd
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import SGDClassifier
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score

#discard lines with more than n zeros in column range
def delete_zeros(X, n, from_column, to_column):
    if to_column<=from_column:
        print("to_column<=from_column!!")
        return 0
    if to_column>X.shape[1]:
        print("to_column>X_columns!!")
        return 0
    list_1=[]
    for row in range(X.shape[0]):
        n_zeros=0
        for column in range(from_column,to_column+1):
            if X[row,column] == 0:
                n_zeros+=1
        if n_zeros > n:
            list_1.append(row)
    X__ = np.delete(X,list_1,axis=0)  
    return X__


'''
perform gradient descend with matrix X_1, true values y, stop when gradient is 
lower than epsilon and set lerning rate

prints number of iteration
output of the function are just betas that are used for X_test to evaluate model

function includes debug option for additional printout

'''
def our_gradient_descent_beta(X_1, y, epsilon, learning_rate):
    N_printout = 10000
    debug = False
    n_iterations = 0
    betas=np.ones(X_1.shape[1])*0.001
    #print(betas.shape)
    gradient_norm = 2*epsilon
    p = np.zeros((X_1.shape[0]))
    #print(p)
    #print(y.shape)
    #print(p.shape)
    while gradient_norm > epsilon:      
        for i in range(X_1.shape[0]):
            if debug:
                if i%N_printout == 0: 
                    print("multiplication = ", X_1[i,:].dot(betas))
                if i%N_printout == 0:
                    print("exp = ", np.exp(X_1[i,:].dot(betas)))
            p[i] = (np.exp((X_1[i,:]).dot(betas)))/(1+np.exp((X_1[i,:]).dot(betas)))
            if debug:
                if i%N_printout == 0: 
                    print("p["+str(i)+"] =", p[i])
        if debug:
            print("transpose X_1", np.transpose(X_1))
            print(np.transpose(X_1).shape)
        ###print("y-p= ", y-p)
        #print(y.shape)
        gradient = (-(np.transpose(X_1))).dot(y-p)
        #print("gradient =", gradient)
        #print("betas = ", betas)
        betas = betas - learning_rate*gradient
        #print("betas = ", betas)
        gradient_norm = np.linalg.norm(gradient)
        ###print("gradient_norm = ", gradient_norm)
        n_iterations = n_iterations +1
    print("Converged in "+str(n_iterations)+" iterations.")
    return betas


#set a random seed
np.random.seed(0)

#read data and create a dataframe
cwd = os.getcwd()
filename = cwd + '/default of credit card clients.xls'
nanDict = {}
df = pd.read_excel(filename, header=1, skiprows=0, index_col=0, na_values=nanDict)

df.rename(index=str, columns={"default payment next month": "defaultPaymentNextMonth"}, inplace=True)

# Deleting rows containing wrong values
sex = [1,2]
education = [1,2,3,4]
marriage = [1,2,3]
payment = [0,1]
pay = [-2,-1,0,1,2,3,4,5,6,7,8,9]

df = df[df['SEX'].isin(sex)]
df = df[df['EDUCATION'].isin(education)]
df = df[df['MARRIAGE'].isin(marriage)]
df = df[df['defaultPaymentNextMonth'].isin(payment)]
df = df[df['PAY_0'].isin(pay)]
df = df[df['PAY_2'].isin(pay)]
df = df[df['PAY_3'].isin(pay)]
df = df[df['PAY_4'].isin(pay)]
df = df[df['PAY_5'].isin(pay)]
df = df[df['PAY_6'].isin(pay)]

#print(df)

# =============================================================================
# # Replacement of null values
# df = df.replace({'PAY_0': {-2: np.nan, 0: np.nan}})
# df = df.replace({'PAY_2': {-2: np.nan, 0: np.nan}})
# df = df.replace({'PAY_3': {-2: np.nan, 0: np.nan}})
# df = df.replace({'PAY_4': {-2: np.nan, 0: np.nan}})
# df = df.replace({'PAY_5': {-2: np.nan, 0: np.nan}})
# df = df.replace({'PAY_6': {-2: np.nan, 0: np.nan}})
# =============================================================================

df = df.replace({'PAY_0': {-2: 0}})
df = df.replace({'PAY_2': {-2: 0}})
df = df.replace({'PAY_3': {-2: 0}})
df = df.replace({'PAY_4': {-2: 0}})
df = df.replace({'PAY_5': {-2: 0}})
df = df.replace({'PAY_6': {-2: 0}})

# Standardization
def standardize(df, label):
    """
    standardizes a series with name ``label'' within the pd.DataFrame
    ``df''.
    """
    df = df.copy(deep=True)
    series = df.loc[:, label]
    avg = series.mean()
    stdv = series.std()
    series_standardized = (series - avg)/ stdv
    return series_standardized

# =============================================================================
# num_col_list = ["LIMIT_BAL","AGE","BILL_AMT1","BILL_AMT2","BILL_AMT3", "BILL_AMT4",
#                 "BILL_AMT5", "BILL_AMT6","PAY_AMT1","PAY_AMT2","PAY_AMT3","PAY_AMT4",
#                 "PAY_AMT5","PAY_AMT6"]
# 
# for label in num_col_list:
#     a=label.index
#     df.a=standardize(df,label)
# =============================================================================
    
df.LIMIT_BAL = standardize(df,"LIMIT_BAL")
df.AGE = standardize(df,"AGE")
df.BILL_AMT1 =standardize(df,"BILL_AMT1")
df.BILL_AMT2 =standardize(df,"BILL_AMT2")
df.BILL_AMT3 =standardize(df,"BILL_AMT3")
df.BILL_AMT4 =standardize(df,"BILL_AMT4")
df.BILL_AMT5 =standardize(df,"BILL_AMT5")
df.BILL_AMT6 =standardize(df,"BILL_AMT6")
df.PAY_AMT1 =standardize(df,"PAY_AMT1")
df.PAY_AMT2 =standardize(df,"PAY_AMT2")
df.PAY_AMT3 =standardize(df,"PAY_AMT3")
df.PAY_AMT4 =standardize(df,"PAY_AMT4")
df.PAY_AMT5 =standardize(df,"PAY_AMT5")
df.PAY_AMT6 =standardize(df,"PAY_AMT6")



print(df)

# Features and targets 
X = df.loc[:, df.columns != 'defaultPaymentNextMonth'].values
y = df.loc[:, df.columns == 'defaultPaymentNextMonth'].values
y = np.ravel(y)
# =============================================================================
# 
# 
# list_1=[]
# for j in range(X.shape[0]):
#     if X[j,1] != 1 and X[j,1] != 2:
#         list_1.append(j)
#         #X=np.delete(X,(j),axis=0) 
#     if X[j,2] != 1 and X[j,2] != 2 and X[j,2] != 3 and X[j,2] != 4:
#         list_1.append(j)
#         #X=np.delete(X,(j),axis=0)
#     if X[j,3] != 1 and X[j,3] != 2 and X[j,3] != 3:
#        list_1.append(j)
#        # X=np.delete(X,(j),axis=0)
#   
# X = np.delete(X,list_1,axis=0)     
#        
# =============================================================================
#print(X.shape)

# =============================================================================
# # Features and targets 
# X = df.loc[:, df.columns != 'defaultPaymentNextMonth'].values
# y = df.loc[:, df.columns == 'defaultPaymentNextMonth'].values
# y = np.ravel(y)
# =============================================================================

onehotencoder = OneHotEncoder(categories="auto")

categor_col_index_list = [1,2,3,5,6,7,8,9,10]

X = ColumnTransformer(
    [("", onehotencoder, [1,2,3]),],
    remainder="passthrough"
).fit_transform(X)

#print(pd.DataFrame(X[:,:16]))

X = ColumnTransformer(
    [("", onehotencoder, [11,12,13,14,15,16]),],
    remainder="passthrough"
).fit_transform(X)

'''
every time zero value in PAY is encoded by onehotencoding, put there zero instead 
because we want to work with only good labeled values

this is made on positions list_bad
'''

list_bad=[]
for i in range(0,6):
    list_bad.append(1+10*i)
    X[:,1+10*i]=np.zeros(X.shape[0])
print(list_bad)

#print(pd.DataFrame(X[:,58:67]))

# =============================================================================
# def standardize(X, label):
#     """
#     standardizes a series with name ``label'' within the pd.DataFrame
#     ``df''.
#     """
#     avg = X[:,label].mean()
#     stdv = X[:,label].std()
#     X_standardized = (X[:,label] - avg)/ stdv
#     return X_standardized
# 
# for i in range(9,29):
#     X[:,i] = standardize(X,i)
# 
# y.shape
# =============================================================================
#build design matrix
n_columns = X.shape[1] + 1
X_1 = np.zeros((X.shape[0],n_columns))
X_1[:,0] = np.ones((X.shape[0]))
X_1[:,1:] = X
#print(pd.DataFrame(X_1[:,:11]))
X_1 = delete_zeros(X_1,10,70,81)

print(pd.DataFrame(X_1))

# Train-test split
trainingShare = 0.8
seed  = 1
XTrain, XTest, yTrain, yTest=train_test_split(X_1, y, train_size=trainingShare, \
                                              test_size = 1-trainingShare,
                                             random_state=seed)


#gradient descent method to estimate betas

learning_rate = 1e-5
epsilon = 100    

betas = our_gradient_descent_beta(XTrain, yTrain, epsilon, learning_rate) 
   
# =============================================================================
# print(betas.shape, X_1.shape)
# print(p.shape)
#print(betas)
    
# =============================================================================
# #another code for gradient descent
# eta = 0.0001 # This is out eta
# Niteration = 100
# beta = np.random.randn(XTrain.shape[1])
# #print(beta)
# for iter in range(Niteration):
#     sigmoid = 1/(1+np.exp(-(XTrain)@(beta)))
#     gradients =  -(np.transpose(XTrain)@(yTrain-sigmoid))
#     print(gradients)
#     beta -= eta*gradients
# #print(beta)
# sigmoid_a = 1/(1+np.exp(-(XTest)@(beta)))
# p = np.round(sigmoid_a)
# 
# =============================================================================

p = np.zeros((XTest.shape[0]))
for i in range(XTest.shape[0]):
    p[i] = (np.exp((XTest[i,:]).dot(betas)))/(1+np.exp((XTest[i,:]).dot(betas)))

p = np.round(p)
# =============================================================================
# print(p.shape)
# print(yTest.shape)
# print(XTest.shape)
# print(XTrain.shape)
# print(X_1.shape)
# =============================================================================

#print(p, yTrain)


accuracy_numer=0
for i in range(p.shape[0]):
    if p[i] == yTest[i]:
        accuracy_numer+=1
accuracy = accuracy_numer/p.shape[0]
print(accuracy)
        
        

# =============================================================================
# clf = SGDClassifier(loss="log", max_iter=5).fit(X_1, y)
# clf.predict_proba(X_1)  
# =============================================================================

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score 

model = LogisticRegression()
model.fit(XTrain, yTrain)
predicted_classes = model.predict(XTest)
print(predicted_classes)
#print(y)
accuracy = accuracy_score(yTest.flatten(),predicted_classes)
parameters = model.coef_
print(accuracy)
