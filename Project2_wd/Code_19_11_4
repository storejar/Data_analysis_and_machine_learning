#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Nov  2 16:52:12 2019

@author: Ary
"""

import pandas as pd
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import SGDClassifier
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score
from sklearn import metrics

#discard lines with more than n zeros in column range
def delete_zeros(X, n, from_column, to_column):
    #X = np.asmatrix(X)
    if to_column<=from_column:
        print("to_column<=from_column!!")
        return 0
    if to_column>X.shape[1]:
        print("to_column>X_columns!!")
        return 0
    list_1=[]
    for row in range(X.shape[0]):
        n_zeros=0
        for column in range(from_column,to_column+1):
            if X.iloc[row, column] == 0:
                n_zeros+=1
        if n_zeros > n:
            list_1.append(row)
    #print(df.index[list_1])
    X__ = X.drop(X.index[list_1])  
    return X__


'''
perform gradient descend with matrix X_1, true values y, stop when gradient is 
lower than epsilon and set lerning rate
prints number of iteration
output of the function are just betas that are used for X_test to evaluate model
function includes debug option for additional printout
'''
def our_gradient_descent_beta(X_1, y, epsilon, learning_rate):
    N_printout = 10000
    debug = False
    n_iterations = 0
    betas=np.ones(X_1.shape[1])*0.001
    #print(betas.shape)
    gradient_norm = 2*epsilon
    p = np.zeros((X_1.shape[0]))
    #print(p)
    #print(y.shape)
    #print(p.shape)
    while gradient_norm > epsilon:      
        for i in range(X_1.shape[0]):
            if debug:
                if i%N_printout == 0: 
                    print("multiplication = ", X_1[i,:].dot(betas))
                if i%N_printout == 0:
                    print("exp = ", np.exp(X_1[i,:].dot(betas)))
            p[i] = (np.exp((X_1[i,:]).dot(betas)))/(1+np.exp((X_1[i,:]).dot(betas)))
            if debug:
                if i%N_printout == 0: 
                    print("p["+str(i)+"] =", p[i])
        if debug:
            print("transpose X_1", np.transpose(X_1))
            print(np.transpose(X_1).shape)
        ###print("y-p= ", y-p)
        #print(y.shape)
        gradient = (-(np.transpose(X_1))).dot(y-p)
        #print("gradient =", gradient)
        #print("betas = ", betas)
        betas = betas - learning_rate*gradient
        #print("betas = ", betas)
        gradient_norm = np.linalg.norm(gradient)
        ###print("gradient_norm = ", gradient_norm)
        n_iterations = n_iterations +1
    print("Converged in "+str(n_iterations)+" iterations.")
    return betas

# stochastic gradient descent code
    
def stochastic_GD(X_1, y, eta, n, M, n_epochs):
    betas=np.ones(X_1.shape[1])*0.001
    m = int(n/M)
    for epoch in range(1,n_epochs+1):
        for i in range(m):
            k = np.random.randint(m)
            xi = X_1[k:k+int(X_1.shape[0]/m)]
            yi = y[k:k+int(y.shape[0]/m)]
            p = np.zeros((xi.shape[0]))
            for i in range(xi.shape[0]):
                p[i] = (np.exp((xi[i,:]).dot(betas)))/(1+np.exp((xi[i,:]).dot(betas)))
            gradient = (-(np.transpose(xi))).dot(yi-p)
            betas = betas - learning_rate*gradient
    return betas

# calculate edges of bins for plotting 2D histogram from used values lmbd_vals
def calc_edges(lmbd_vals):
    border=3
    edges=np.zeros(len(lmbd_vals)+1)
    edges[0]=border*lmbd_vals[0]/10
    for i in range(1,len(edges)):
        edges[i]=border*lmbd_vals[i-1]
    #print(edges)
    return edges


#set a random seed
np.random.seed(0)

#read data and create a dataframe
cwd = os.getcwd()
filename = cwd + '/default of credit card clients.xls'
nanDict = {}
df = pd.read_excel(filename, header=1, skiprows=0, index_col=0, na_values=nanDict)

df.rename(index=str, columns={"default payment next month": "defaultPaymentNextMonth"}, inplace=True)

# Deleting rows containing wrong values
sex = [1,2]
education = [1,2,3,4]
marriage = [1,2,3]
payment = [0,1]
pay = [-2,-1,0,1,2,3,4,5,6,7,8,9]

df = df[df['SEX'].isin(sex)]
df = df[df['EDUCATION'].isin(education)]
df = df[df['MARRIAGE'].isin(marriage)]
df = df[df['defaultPaymentNextMonth'].isin(payment)]
df = df[df['PAY_0'].isin(pay)]
df = df[df['PAY_2'].isin(pay)]
df = df[df['PAY_3'].isin(pay)]
df = df[df['PAY_4'].isin(pay)]
df = df[df['PAY_5'].isin(pay)]
df = df[df['PAY_6'].isin(pay)]

print(df.shape)

# =============================================================================
# # Replacement of null values
# df = df.replace({'PAY_0': {-2: np.nan, 0: np.nan}})
# df = df.replace({'PAY_2': {-2: np.nan, 0: np.nan}})
# df = df.replace({'PAY_3': {-2: np.nan, 0: np.nan}})
# df = df.replace({'PAY_4': {-2: np.nan, 0: np.nan}})
# df = df.replace({'PAY_5': {-2: np.nan, 0: np.nan}})
# df = df.replace({'PAY_6': {-2: np.nan, 0: np.nan}})
# =============================================================================

df = df.replace({'PAY_0': {-2: 0}})
df = df.replace({'PAY_2': {-2: 0}})
df = df.replace({'PAY_3': {-2: 0}})
df = df.replace({'PAY_4': {-2: 0}})
df = df.replace({'PAY_5': {-2: 0}})
df = df.replace({'PAY_6': {-2: 0}})

#print(df)

df = delete_zeros(df,10,df.columns.get_loc("BILL_AMT1"),df.columns.get_loc("PAY_AMT6"))
#print(df)
print(df)
#print(df.columns.get_loc("BILL_AMT1"))
#print(df.LIMIT_BAL)

# Standardization
def standardize(df, label):
    """
    standardizes a series with name ``label'' within the pd.DataFrame
    ``df''.
    """
    df = df.copy(deep=True)
    series = df.loc[:, label]
    avg = series.mean()
    stdv = series.std()
    series_standardized = (series - avg)/ stdv
    return series_standardized

# =============================================================================
# num_col_list = ["LIMIT_BAL","AGE","BILL_AMT1","BILL_AMT2","BILL_AMT3", "BILL_AMT4",
#                 "BILL_AMT5", "BILL_AMT6","PAY_AMT1","PAY_AMT2","PAY_AMT3","PAY_AMT4",
#                 "PAY_AMT5","PAY_AMT6"]
# 
# for label in num_col_list:
#     a=label.index
#     df.a=standardize(df,label)
# =============================================================================
    
df = pd.DataFrame(df)
print(df)
df.LIMIT_BAL = standardize(df,"LIMIT_BAL")
df.AGE = standardize(df,"AGE")
df.BILL_AMT1 =standardize(df,"BILL_AMT1")
df.BILL_AMT2 =standardize(df,"BILL_AMT2")
df.BILL_AMT3 =standardize(df,"BILL_AMT3")
df.BILL_AMT4 =standardize(df,"BILL_AMT4")
df.BILL_AMT5 =standardize(df,"BILL_AMT5")
df.BILL_AMT6 =standardize(df,"BILL_AMT6")
df.PAY_AMT1 =standardize(df,"PAY_AMT1")
df.PAY_AMT2 =standardize(df,"PAY_AMT2")
df.PAY_AMT3 =standardize(df,"PAY_AMT3")
df.PAY_AMT4 =standardize(df,"PAY_AMT4")
df.PAY_AMT5 =standardize(df,"PAY_AMT5")
df.PAY_AMT6 =standardize(df,"PAY_AMT6")



print(df)

# Features and targets 
X = df.loc[:, df.columns != 'defaultPaymentNextMonth'].values
y = df.loc[:, df.columns == 'defaultPaymentNextMonth'].values
y = np.ravel(y)
# =============================================================================
# 
# 
# list_1=[]
# for j in range(X.shape[0]):
#     if X[j,1] != 1 and X[j,1] != 2:
#         list_1.append(j)
#         #X=np.delete(X,(j),axis=0) 
#     if X[j,2] != 1 and X[j,2] != 2 and X[j,2] != 3 and X[j,2] != 4:
#         list_1.append(j)
#         #X=np.delete(X,(j),axis=0)
#     if X[j,3] != 1 and X[j,3] != 2 and X[j,3] != 3:
#        list_1.append(j)
#        # X=np.delete(X,(j),axis=0)
#   
# X = np.delete(X,list_1,axis=0)     
#        
# =============================================================================
#print(X.shape)

# =============================================================================
# # Features and targets 
# X = df.loc[:, df.columns != 'defaultPaymentNextMonth'].values
# y = df.loc[:, df.columns == 'defaultPaymentNextMonth'].values
# y = np.ravel(y)
# =============================================================================
import seaborn as sns
import matplotlib.pyplot as plt
fig1=plt.figure(figsize=(20,15))
# use the heatmap function from seaborn to plot the correlation matrix
# annot = True to print the values inside the square
ax0=sns.heatmap(data=df.corr().round(2), annot=True)
bottom, top = ax0.get_ylim()
ax0.set_ylim(bottom + 0.5, top - 0.5)
plt.title(r'Correlation heatmap', fontsize=25)
plt.show()
fig1.savefig('corr_matrix.png')
plt.close()


onehotencoder = OneHotEncoder(categories="auto")

categor_col_index_list = [1,2,3,5,6,7,8,9,10]

X = ColumnTransformer(
    [("", onehotencoder, [1,2,3]),],
    remainder="passthrough"
).fit_transform(X)

#print(pd.DataFrame(X[:,:16]))

X = ColumnTransformer(
    [("", onehotencoder, [11,12,13,14,15,16]),],
    remainder="passthrough"
).fit_transform(X)

'''
every time zero value in PAY is encoded by onehotencoding, put there zero instead 
because we want to work with only good labeled values
this is made on positions list_bad
'''

list_bad=[]
for i in range(0,6):
    list_bad.append(1+10*i)
    X[:,1+10*i]=np.zeros(X.shape[0])
print(list_bad)

#print(pd.DataFrame(X[:,58:67]))

# =============================================================================
# def standardize(X, label):
#     """
#     standardizes a series with name ``label'' within the pd.DataFrame
#     ``df''.
#     """
#     avg = X[:,label].mean()
#     stdv = X[:,label].std()
#     X_standardized = (X[:,label] - avg)/ stdv
#     return X_standardized
# 
# for i in range(9,29):
#     X[:,i] = standardize(X,i)
# 
# y.shape
# =============================================================================
#build design matrix
#n_columns = X.shape[1] + 1
#X_1 = np.zeros((X.shape[0],n_columns))
#X_1[:,0] = np.ones((X.shape[0]))
X_1 = X
#print(pd.DataFrame(X_1[:,:11]))


print(pd.DataFrame(X_1))

# Train-test split
trainingShare = 0.8
seed  = 1
XTrain, XTest, yTrain, yTest=train_test_split(X_1, y, train_size=trainingShare, \
                                              test_size = 1-trainingShare,
                                             random_state=seed)


#gradient descent method to estimate betas

learning_rate = 1e-5
epsilon = 100    

betas = our_gradient_descent_beta(XTrain, yTrain, epsilon, learning_rate) 
betas_st = stochastic_GD(XTrain, yTrain, learning_rate, XTrain.shape[0], 5, 500)
   
# =============================================================================
# print(betas.shape, X_1.shape)
# print(p.shape)
#print(betas)
    
# =============================================================================
# #another code for gradient descent
# eta = 0.0001 # This is out eta
# Niteration = 100
# beta = np.random.randn(XTrain.shape[1])
# #print(beta)
# for iter in range(Niteration):
#     sigmoid = 1/(1+np.exp(-(XTrain)@(beta)))
#     gradients =  -(np.transpose(XTrain)@(yTrain-sigmoid))
#     print(gradients)
#     beta -= eta*gradients
# #print(beta)
# sigmoid_a = 1/(1+np.exp(-(XTest)@(beta)))
# p = np.round(sigmoid_a)
# 
# =============================================================================

p = np.zeros((XTest.shape[0]))
for i in range(XTest.shape[0]):
    p[i] = (np.exp((XTest[i,:]).dot(betas)))/(1+np.exp((XTest[i,:]).dot(betas)))

p = np.round(p)

p_st = np.zeros((XTest.shape[0]))
for i in range(XTest.shape[0]):
    p_st[i] = (np.exp((XTest[i,:]).dot(betas_st)))/(1+np.exp((XTest[i,:]).dot(betas_st)))

p_st = np.round(p_st)

f1_simple_gd, t1_simple_gd, _ = metrics.roc_curve(yTest, p)
auc1_simple_gd = metrics.roc_auc_score(yTest, p)
print(auc1_simple_gd)

f1_st_gd, t1_st_gd, _ = metrics.roc_curve(yTest, p_st)
auc1_st_gd = metrics.roc_auc_score(yTest, p_st)
print(auc1_st_gd)
# =============================================================================
# print(p.shape)
# print(yTest.shape)
# print(XTest.shape)
# print(XTrain.shape)
# print(X_1.shape)
# =============================================================================

#print(p, yTrain)


def accuracyscore(p,yTest):
    accuracy_numer=0
    for i in range(p.shape[0]):
        if p[i] == yTest[i]:
            accuracy_numer+=1
    accuracy = accuracy_numer/p.shape[0]
    return accuracy
            
acc = accuracyscore(p,yTest)  
print(acc)
acc_st = accuracyscore(p_st,yTest)
print(acc_st)
        

# =============================================================================
# clf = SGDClassifier(loss="log", max_iter=5).fit(X_1, y)
# clf.predict_proba(X_1)  
# =============================================================================

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score 

model = LogisticRegression()
model.fit(XTrain, yTrain)
predicted_classes = model.predict(XTest)
print(predicted_classes)
#print(y)
accuracy = accuracy_score(yTest.flatten(),predicted_classes)
parameters = model.coef_
print(accuracy)

def sigmoid(x):
    return 1/(1 + np.exp(-x))

from sklearn.metrics import accuracy_score

# one-hot in numpy
def to_categorical_numpy(integer_vector):
    n_inputs = len(integer_vector)
    n_categories = np.max(integer_vector) + 1
    onehot_vector = np.zeros((n_inputs, n_categories))
    onehot_vector[range(n_inputs), integer_vector] = 1
    
    return onehot_vector

#Y_train_onehot, Y_test_onehot = to_categorical(Y_train), to_categorical(Y_test)
Y_train_onehot, Y_test_onehot = to_categorical_numpy(yTrain), to_categorical_numpy(yTest)

# building our neural network

n_inputs, n_features = XTrain.shape
n_hidden_neurons1 = 50
n_hidden_neurons2 = 40
n_hidden_neurons3 = 30
n_categories = 2

# we make the weights normally distributed using numpy.random.randn

# weights and bias in the hidden layer
hidden_weights1 = np.random.randn(n_features, n_hidden_neurons1)
hidden_bias1 = np.zeros(n_hidden_neurons1) + 0.01

hidden_weights2 = np.random.randn(n_hidden_neurons1, n_hidden_neurons2)
hidden_bias2 = np.zeros(n_hidden_neurons2) + 0.01

hidden_weights3 = np.random.randn(n_hidden_neurons2, n_hidden_neurons3)
hidden_bias3 = np.zeros(n_hidden_neurons3) + 0.01

# weights and bias in the output layer
output_weights = np.random.randn(n_hidden_neurons3, n_categories)
output_bias = np.zeros(n_categories) + 0.01

def feed_forward(X):
    # weighted sum of inputs to the hidden layer
    z_h1 = np.matmul(X, hidden_weights1) + hidden_bias1
    # activation in the hidden layer
    a_h1 = sigmoid(z_h1)
    
    z_h2 = np.matmul(a_h1, hidden_weights2) + hidden_bias2
    # activation in the hidden layer
    a_h2 = sigmoid(z_h2)
    
    z_h3 = np.matmul(a_h2, hidden_weights3) + hidden_bias3
    # activation in the hidden layer
    a_h3 = sigmoid(z_h3)
    
    # weighted sum of inputs to the output layer
    z_o = np.matmul(a_h3, output_weights) + output_bias
    # softmax output
    # axis 0 holds each input and axis 1 the probabilities of each category
    exp_term = np.exp(z_o)
    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)
    
    return probabilities

probabilities = feed_forward(XTrain)
print("probabilities = (n_inputs, n_categories) = " + str(probabilities.shape))
print("probability that image 0 is in category 0,1,2,...,9 = \n" + str(probabilities[0]))
print("probabilities sum up to: " + str(probabilities[0].sum()))
print()

# we obtain a prediction by taking the class with the highest likelihood
def predict(X):
    probabilities = feed_forward(X)
    return np.argmax(probabilities, axis=1)

predictions = predict(XTrain)
print("predictions = (n_inputs) = " + str(predictions.shape))
print("prediction for image 0: " + str(predictions[0]))
print("correct label for image 0: " + str(yTrain[0]))

def feed_forward_train(X):
# weighted sum of inputs to the hidden layer
    z_h1 = np.matmul(X, hidden_weights1) + hidden_bias1
    # activation in the hidden layer
    a_h1 = sigmoid(z_h1)
    #print(a_h1.shape)
    
    z_h2 = np.matmul(a_h1, hidden_weights2) + hidden_bias2
    # activation in the hidden layer
    a_h2 = sigmoid(z_h2)
   # print(a_h2.shape)
    
    z_h3 = np.matmul(a_h2, hidden_weights3) + hidden_bias3
    # activation in the hidden layer
    a_h3 = sigmoid(z_h3)
    
    # weighted sum of inputs to the output layer
    z_o = np.matmul(a_h3, output_weights) + output_bias
    # axis 0 holds each input and axis 1 the probabilities of each category
    exp_term = np.exp(z_o)
    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)
    
    # for backpropagation need activations in hidden and output layers
    return a_h1, a_h2, a_h3, probabilities

def backpropagation(X, Y):
    a_h1, a_h2, a_h3, probabilities = feed_forward_train(X)

    
    # error in the output layer
    error_output = probabilities - Y
    # error in the hidden layer
    error_hidden3 = np.matmul(error_output, output_weights.T) * a_h3 * (1 - a_h3)
      
    # gradients for the output layer
    output_weights_gradient = np.matmul(a_h3.T, error_output)
    output_bias_gradient = np.sum(error_output, axis=0)
    
    # gradient for the hidden layer
    hidden_weights_gradient3 = np.matmul(a_h2.T, error_hidden3)
    hidden_bias_gradient3 = np.sum(error_hidden3, axis=0)
    
    error_hidden2 = np.matmul(error_hidden3, hidden_weights3.T) * a_h2 * (1 - a_h2)
    hidden_weights_gradient2 = np.matmul(a_h1.T, error_hidden2)
    hidden_bias_gradient2 = np.sum(error_hidden2, axis=0)
    
    error_hidden1 = np.matmul(error_hidden2, hidden_weights2.T) * a_h1 * (1 - a_h1)
    hidden_weights_gradient1 = np.matmul(X.T, error_hidden1)
    hidden_bias_gradient1 = np.sum(error_hidden1, axis=0)
    
    return output_weights_gradient, output_bias_gradient, hidden_weights_gradient1, hidden_bias_gradient1,hidden_weights_gradient2, hidden_bias_gradient2,hidden_weights_gradient3, hidden_bias_gradient3


print("Old accuracy on training data: " + str(accuracy_score(predict(XTrain), yTrain)))

eta = 0.01
lmbd = 0.01
for i in range(100):
    # calculate gradients
    dWo, dBo, dWh1, dBh1, dWh2, dBh2, dWh3, dBh3 = backpropagation(XTrain, Y_train_onehot)
    
    # regularization term gradients
    dWo += lmbd * output_weights
    dWh1 += lmbd * hidden_weights1
    dWh2 += lmbd * hidden_weights2
    dWh3 += lmbd * hidden_weights3
    
    
    # update weights and biases
    output_weights -= eta * dWo
    output_bias -= eta * dBo
    hidden_weights1 -= eta * dWh1
    hidden_bias1 -= eta * dBh1
    hidden_weights2 -= eta * dWh2
    hidden_bias2 -= eta * dBh2
    hidden_weights3 -= eta * dWh3
    hidden_bias3 -= eta * dBh3
    

print("New accuracy on training data: " + str(accuracy_score(predict(XTrain), yTrain)))

test_predict = predict(XTest)
# =============================================================================
# print(test_predict.shape)
# print(yTest.shape)
# =============================================================================
print("Accuracy score on test set: ", accuracy_score(yTest, test_predict))

from sklearn.neural_network import MLPClassifier
eta_vals = np.logspace(-5, 1, 7)
lmbd_vals = np.logspace(-5, 1, 7)

#make 3 lists for saving histogram values
list_eta_our=[]
list_lmbd_our=[]
list_accur_our=[]
list_predictions=[]
list_auc_our=[]

for i, eta in enumerate(eta_vals):
    for j, lmbd in enumerate(lmbd_vals):
        hidden_weights1 = np.random.randn(n_features, n_hidden_neurons1)
        hidden_bias1 = np.zeros(n_hidden_neurons1) + 0.01

        hidden_weights2 = np.random.randn(n_hidden_neurons1, n_hidden_neurons2)
        hidden_bias2 = np.zeros(n_hidden_neurons2) + 0.01

        hidden_weights3 = np.random.randn(n_hidden_neurons2, n_hidden_neurons3)
        hidden_bias3 = np.zeros(n_hidden_neurons3) + 0.01

# weights and bias in the output layer
        output_weights = np.random.randn(n_hidden_neurons3, n_categories)
        output_bias = np.zeros(n_categories) + 0.01
        
        for i in range(100):
            # calculate gradients
            dWo, dBo, dWh1, dBh1, dWh2, dBh2, dWh3, dBh3 = backpropagation(XTrain, Y_train_onehot)
            
            # regularization term gradients
            dWo += lmbd * output_weights
            dWh1 += lmbd * hidden_weights1
            dWh2 += lmbd * hidden_weights2
            dWh3 += lmbd * hidden_weights3
            
            
            # update weights and biases
            output_weights -= eta * dWo
            output_bias -= eta * dBo
            hidden_weights1 -= eta * dWh1
            hidden_bias1 -= eta * dBh1
            hidden_weights2 -= eta * dWh2
            hidden_bias2 -= eta * dBh2
            hidden_weights3 -= eta * dWh3
            hidden_bias3 -= eta * dBh3
        
        #DNN_numpy[i][j] = dnn
        
        test_predict = predict(XTest)
        f1_nn, t1_nn, _ = metrics.roc_curve(yTest, test_predict)
        auc1_nn = metrics.roc_auc_score(yTest, test_predict)
        
        print("Learning rate  = ", eta)
        print("Lambda = ", lmbd)
        print("Accuracy score on test set: ", accuracy_score(yTest, test_predict))
        print("AUC score on test set: ", auc1_nn)
        print()
        
        list_predictions.append(test_predict)
        list_eta_our.append(eta)
        list_lmbd_our.append(lmbd)
        list_accur_our.append(accuracy_score(yTest, test_predict))
        list_auc_our.append(auc1_nn)
        
        
print(max(list_accur_our))
print(max(list_auc_our))
        
        
#plot histogram        
edges_lmbd = calc_edges(lmbd_vals)
edges_eta = calc_edges(eta_vals)

plt.rc('font', size=20)
fig1=plt.figure(figsize=(12,9))
fig1, ax1 = plt.subplots(figsize=(12,9))
h1=plt.hist2d(list_lmbd_our, list_eta_our, weights=list_accur_our, bins=(edges_lmbd,edges_eta))
ax1.set_aspect("equal")
hist, xbins, ybins, im = ax1.hist2d(list_lmbd_our, list_eta_our, weights=list_accur_our, bins=(edges_lmbd,edges_eta))
scale_hist=3 #position of numbers in histogram fields
for i in range(len(ybins)-1):
    for j in range(len(xbins)-1):
        ax1.text(scale_hist*xbins[j],scale_hist*ybins[i], round(hist[j,i],2), 
                color="w", ha="center", va="center", fontweight="bold", fontsize=15)
plt.colorbar(h1[3])
plt.ylabel(r'Learning rate')
plt.xlabel(r'Lambda')
plt.title(r'Accuracy of our code')
plt.xscale('log')
plt.yscale('log')
fig1.savefig('AccuracyVsEtaVsLambda_our.png')
plt.show()
plt.close()

fig11=plt.figure(figsize=(12,9))
fig11, ax11 = plt.subplots(figsize=(12,9))
h11=plt.hist2d(list_lmbd_our, list_eta_our, weights=list_auc_our, bins=(edges_lmbd,edges_eta))
ax11.set_aspect("equal")
hist, xbins, ybins, im = ax11.hist2d(list_lmbd_our, list_eta_our, weights=list_auc_our, bins=(edges_lmbd,edges_eta))
scale_hist=3 #position of numbers in histogram fields
for i in range(len(ybins)-1):
    for j in range(len(xbins)-1):
        ax11.text(scale_hist*xbins[j],scale_hist*ybins[i], round(hist[j,i],2), 
                color="w", ha="center", va="center", fontweight="bold", fontsize=15)
plt.colorbar(h11[3])
plt.ylabel(r'Learning rate')
plt.xlabel(r'Lambda')
plt.title(r'AUC of our code')
plt.xscale('log')
plt.yscale('log')
fig11.savefig('AUCVsEtaVsLambda_our.png')
plt.show()
plt.close()

#cumulative gain plots
import scikitplot as skplt
skplt.metrics.plot_calibration_curve(yTest, list_predictions)
plt.show()
plt.close()
        
        

# store models for later use - scikit-learn neural network
DNN_scikit = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)

#make 3 lists again
list_eta=[]
list_lmbd=[]
list_accur=[]
list_auc=[]

for i, eta in enumerate(eta_vals):
    for j, lmbd in enumerate(lmbd_vals):
        dnn = MLPClassifier(hidden_layer_sizes=(n_hidden_neurons1, n_hidden_neurons2, n_hidden_neurons3), activation='logistic',
                            alpha=lmbd, learning_rate_init=eta, max_iter=100)
        dnn.fit(XTrain, yTrain)
        
        DNN_scikit[i][j] = dnn
        
        predictions=dnn.predict(XTest)
        f1_nn_sl, t1_nn_sl, _ = metrics.roc_curve(yTest, predictions)
        auc1_nn_sl = metrics.roc_auc_score(yTest, predictions)
        
        print("Learning rate  = ", eta)
        print("Lambda = ", lmbd)
        print("Accuracy score on test set: ", dnn.score(XTest, yTest))
        print("AUC score on test set: ", auc1_nn_sl)
        print()
        
        list_eta.append(eta)
        list_lmbd.append(lmbd)
        list_accur.append(dnn.score(XTest, yTest))
        list_auc.append(auc1_nn_sl)

print(max(list_accur))
print(max(list_auc))
        
        
#plot histogram
edges_lmbd = calc_edges(lmbd_vals)
edges_eta = calc_edges(eta_vals)

plt.rc('font', size=20)
fig2=plt.figure(figsize=(12,9))
fig2, ax2 = plt.subplots(figsize=(12,9))
h2=plt.hist2d(list_lmbd, list_eta, weights=list_accur, bins=(edges_lmbd,edges_eta))
ax2.set_aspect("equal")
hist2, xbins2, ybins2, im2 = ax2.hist2d(list_lmbd, list_eta, weights=list_accur, bins=(edges_lmbd,edges_eta))
scale_hist=3 #position of numbers in histogram fields
for i in range(len(ybins2)-1):
    for j in range(len(xbins2)-1):
        ax2.text(scale_hist*xbins2[j],scale_hist*ybins2[i], round(hist2[j,i],2), 
                color="w", ha="center", va="center", fontweight="bold", fontsize=15)
plt.colorbar(h2[3])
plt.ylabel(r'Learning rate')
plt.xlabel(r'Lambda')
plt.title(r'Accuracy sklearn')
plt.xscale('log')
plt.yscale('log')
fig2.savefig('AccuracyVsEtaVsLambda_skl.png')
plt.show()
plt.close()

fig21=plt.figure(figsize=(12,9))
fig21, ax21 = plt.subplots(figsize=(12,9))
h21=plt.hist2d(list_lmbd, list_eta, weights=list_auc, bins=(edges_lmbd,edges_eta))
ax21.set_aspect("equal")
hist2, xbins2, ybins2, im2 = ax21.hist2d(
list_lmbd, list_eta, weights=list_auc, bins=(edges_lmbd,edges_eta))
scale_hist=3 #position of numbers in histogram fields
for i in range(len(ybins2)-1):
    for j in range(len(xbins2)-1):
        ax21.text(scale_hist*xbins2[j],scale_hist*ybins2[i], round(hist2[j,i],2), 
                color="w", ha="center", va="center", fontweight="bold", fontsize=15)
plt.colorbar(h21[3])
plt.ylabel(r'Learning rate')
plt.xlabel(r'Lambda')
plt.title(r'AUC sklearn')
plt.xscale('log')
plt.yscale('log')
fig21.savefig('AUCVsEtaVsLambda_skl.png')
plt.show()
plt.close()
        
        
        
        
