n_inputs, n_features = XTrain.shape
n_hidden_neurons1 = 50
n_hidden_neurons2 = 40
n_hidden_neurons3 = 30
n_categories = 2

# we make the weights normally distributed using numpy.random.randn

# weights and bias in the hidden layer
np.random.seed(0)
hidden_weights1 = np.random.randn(n_features, n_hidden_neurons1)
hidden_bias1 = np.zeros(n_hidden_neurons1) + 0.01
np.random.seed(0)
hidden_weights2 = np.random.randn(n_hidden_neurons1, n_hidden_neurons2)
hidden_bias2 = np.zeros(n_hidden_neurons2) + 0.01
np.random.seed(0)
hidden_weights3 = np.random.randn(n_hidden_neurons2, n_hidden_neurons3)
hidden_bias3 = np.zeros(n_hidden_neurons3) + 0.01

# weights and bias in the output layer
np.random.seed(0)
output_weights = np.random.randn(n_hidden_neurons3, n_categories)
output_bias = np.zeros(n_categories) + 0.01

def feed_forward(X):
    # weighted sum of inputs to the hidden layer
    z_h1 = np.matmul(X, hidden_weights1) + hidden_bias1
    # activation in the hidden layer
    a_h1 = sigmoid(z_h1)
    
    z_h2 = np.matmul(a_h1, hidden_weights2) + hidden_bias2
    # activation in the hidden layer
    a_h2 = sigmoid(z_h2)
    
    z_h3 = np.matmul(a_h2, hidden_weights3) + hidden_bias3
    # activation in the hidden layer
    a_h3 = sigmoid(z_h3)
    
    # weighted sum of inputs to the output layer
    z_o = np.matmul(a_h3, output_weights) + output_bias
    # softmax output
    # axis 0 holds each input and axis 1 the probabilities of each category
    exp_term = np.exp(z_o)
    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)
    
    return probabilities
    
    def feed_forward_train(X):
# weighted sum of inputs to the hidden layer
    z_h1 = np.matmul(X, hidden_weights1) + hidden_bias1
    # activation in the hidden layer
    a_h1 = sigmoid(z_h1)
    #print(a_h1.shape)
    
    z_h2 = np.matmul(a_h1, hidden_weights2) + hidden_bias2
    # activation in the hidden layer
    a_h2 = sigmoid(z_h2)
   # print(a_h2.shape)
    
    z_h3 = np.matmul(a_h2, hidden_weights3) + hidden_bias3
    # activation in the hidden layer
    a_h3 = sigmoid(z_h3)
    
    # weighted sum of inputs to the output layer
    z_o = np.matmul(a_h3, output_weights) + output_bias
    # axis 0 holds each input and axis 1 the probabilities of each category
    exp_term = np.exp(z_o)
    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)
    
    # for backpropagation need activations in hidden and output layers
    return a_h1, a_h2, a_h3, probabilities

def backpropagation(X, Y):
    a_h1, a_h2, a_h3, probabilities = feed_forward_train(X)

    
    # error in the output layer
    error_output = probabilities - Y
    # error in the hidden layer
    error_hidden3 = np.matmul(error_output, output_weights.T) * a_h3 * (1 - a_h3)
      
    # gradients for the output layer
    output_weights_gradient = np.matmul(a_h3.T, error_output)
    output_bias_gradient = np.sum(error_output, axis=0)
    
    # gradient for the hidden layer
    hidden_weights_gradient3 = np.matmul(a_h2.T, error_hidden3)
    hidden_bias_gradient3 = np.sum(error_hidden3, axis=0)
    
    error_hidden2 = np.matmul(error_hidden3, hidden_weights3.T) * a_h2 * (1 - a_h2)
    hidden_weights_gradient2 = np.matmul(a_h1.T, error_hidden2)
    hidden_bias_gradient2 = np.sum(error_hidden2, axis=0)
    
    error_hidden1 = np.matmul(error_hidden2, hidden_weights2.T) * a_h1 * (1 - a_h1)
    hidden_weights_gradient1 = np.matmul(X.T, error_hidden1)
    hidden_bias_gradient1 = np.sum(error_hidden1, axis=0)
    
    return output_weights_gradient, output_bias_gradient, hidden_weights_gradient1, hidden_bias_gradient1,hidden_weights_gradient2, hidden_bias_gradient2,hidden_weights_gradient3, hidden_bias_gradient3
