#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Nov  5 09:29:17 2019
@author: Ary
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score


import seaborn as sns
import matplotlib.pyplot as plt


np.random.seed(2204)

## part a

def FrankeFunction(x,y):
    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))
    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))
    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))
    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)
    return term1 + term2 + term3 + term4

def Design_Matrix_X(x, y, n):
	N = len(x)
	l = int((n+1)*(n+2)/2)		
	X = np.ones((N,l))

	for i in range(1,n+1):
		q = int((i)*(i+1)/2)
		for k in range(i+1):
			X[:,q+k] = x**(i-k) * y**k

	return X

n_x=1000
m=5

np.random.seed(2204)
x = np.random.uniform(0, 1, n_x)
np.random.seed(2204)
y = np.random.uniform(0, 1, n_x)
#x, y = np.meshgrid(x,y)

z = FrankeFunction(x,y)

print(z.shape)

n = int(len(x))
np.random.seed(2204)
z_1 = z + 0.01*np.random.randn(n)
#z_1 = (z_1-np.mean(z_1))/np.std(z_1)
#z_1, z_1=np.meshgrid(z_1, z_1)
#array = array[:,np.newaxis]
z_1 = z_1[:,np.newaxis]
#z_1=z_1.reshape(1000,1)
print(z_1.shape)
X= Design_Matrix_X(x,y,n=m)
DesignMatrix = pd.DataFrame(X)
print(DesignMatrix.shape)

np.random.seed(2204)
trainingShare = 0.8
seed  = 1
XTrain, XTest, yTrain, yTest=train_test_split(X, z_1, train_size=trainingShare, \
                                              test_size = 1-trainingShare,
                                             random_state=seed)

print(yTrain.shape)

n_inputs, n_features = XTrain.shape
#come scelgo il numero di categorie
n_categories = 1
n_hidden_neurons1 = 50
n_hidden_neurons2 = 40
n_hidden_neurons3 = 30

def relu(X):
   return np.maximum(0,X)

def sigmoid(x):
    return 1/(1 + np.exp(-x))

np.random.seed(2204)
# weights and bias in the hidden layer
hidden_weights1 = np.random.randn(n_features, n_hidden_neurons1)
hidden_bias1 = np.zeros(n_hidden_neurons1) + 0.01
#print(hidden_bias1.shape)
np.random.seed(2204)
hidden_weights2 = np.random.randn(n_hidden_neurons1, n_hidden_neurons2)
hidden_bias2 = np.zeros(n_hidden_neurons2) + 0.01
np.random.seed(2204)
hidden_weights3 = np.random.randn(n_hidden_neurons2, n_hidden_neurons3)
hidden_bias3 = np.zeros(n_hidden_neurons3) + 0.01
np.random.seed(2204)
# weights and bias in the output layer
output_weights = np.random.randn(n_hidden_neurons3, n_categories)
output_bias = np.zeros(n_categories) + 0.01

print(hidden_weights1)
print(hidden_weights2)
print(hidden_weights3)

def feed_forward(X):
    # weighted sum of inputs to the hidden layer
    z_h1 = np.matmul(X, hidden_weights1) + hidden_bias1
    #print(z_h1.shape)
    # activation in the hidden layer
    #a_h1 = relu(z_h1)
    a_h1 = sigmoid(z_h1)
    #print(a_h1.shape)
    
    z_h2 = np.matmul(a_h1, hidden_weights2) + hidden_bias2
    #print(z_h2)
    # activation in the hidden layer
    #a_h2 = relu(z_h2)
    a_h2 = sigmoid(z_h2)
   # print(a_h2)
    
    z_h3 = np.matmul(a_h2, hidden_weights3) + hidden_bias3
    #print(z_h3)
    # activation in the hidden layer
    #a_h3 = relu(z_h3)
    a_h3 = sigmoid(z_h3)
    #sprint(a_h3)
    
    # weighted sum of inputs to the output layer
    z_o = np.matmul(a_h3, output_weights) + output_bias
    a_ho = sigmoid(z_o)
# =============================================================================
#     exp_term = np.exp(z_o)
#     outputs = exp_term / np.sum(exp_term, axis=1, keepdims=True)
# =============================================================================
    #print(z_o) 
    
    outputs = a_ho #is it the right function for output values?

    return outputs


def predict(X):
    outputs = feed_forward(X)
# =============================================================================
#     print(probabilities)
   # a=np.argmax(outputs, axis=1)
# =============================================================================
    #out = (np.sum(outputs, axis=1, keepdims=True))/200
    return outputs

def feed_forward_train(X):

    z_h1 = np.matmul(X, hidden_weights1) + hidden_bias1
    #print(z_h1.shape)
    # activation in the hidden layer
    #a_h1 = relu(z_h1)
    a_h1 = sigmoid(z_h1)
    #print(a_h1.shape)
    
    z_h2 = np.matmul(a_h1, hidden_weights2) + hidden_bias2
    #print(z_h2.shape)
    # activation in the hidden layer
    #a_h2 = relu(z_h2)
    a_h2 = sigmoid(z_h2)
    #print(a_h2.shape)
    
    z_h3 = np.matmul(a_h2, hidden_weights3) + hidden_bias3
    #print(z_h3.shape)
    # activation in the hidden layer
    #a_h3 = relu(z_h3)
    a_h3 = sigmoid(z_h3)
    
    # weighted sum of inputs to the output layer
    z_o = np.matmul(a_h3, output_weights) + output_bias
    a_ho = sigmoid(z_o)
    # axis 0 holds each input and axis 1 the probabilities of each category
    #print(z_o.shape)
    outputs = a_ho
    return a_h1, a_h2, a_h3, a_ho, outputs


def backpropagation(X, Y):
    a_h1, a_h2, a_h3, a_ho, outputs = feed_forward_train(X)

    n = Y.shape[0]
    term = outputs - Y
 #   print(term.shape)
    error_output = ((2/n)*term) * a_ho * (1 - a_ho)
    
# =============================================================================
#     print(error_output.shape)
#     print(output_weights.T.shape)
#     print(np.matmul(error_output, output_weights.T).shape)
#     print(a_h3.shape)
#   #  print(a_h3.shape)
# =============================================================================
    
    error_hidden3 = np.matmul(error_output, output_weights.T) * a_h3 * (1 - a_h3)
    error_hidden2 = np.matmul(error_hidden3, hidden_weights3.T) * a_h2 * (1 - a_h2)
    error_hidden1 = np.matmul(error_hidden2, hidden_weights2.T) * a_h1 * (1 - a_h1)

    output_weights_gradient = np.matmul(a_h3.T, error_output)
    output_bias_gradient = np.sum(error_output, axis=0)
    
    hidden_weights_gradient3 = np.matmul(a_h2.T, error_hidden3)
    hidden_bias_gradient3 = np.sum(error_hidden3, axis=0)
    
    hidden_weights_gradient2 = np.matmul(a_h1.T, error_hidden2)
    hidden_bias_gradient2 = np.sum(error_hidden2, axis=0)
    
    hidden_weights_gradient1 = np.matmul(X.T, error_hidden1)
    hidden_bias_gradient1 = np.sum(error_hidden1, axis=0)
    
    return output_weights_gradient, output_bias_gradient, hidden_weights_gradient1, hidden_bias_gradient1,hidden_weights_gradient2, hidden_bias_gradient2,hidden_weights_gradient3, hidden_bias_gradient3


eta_vals = [1e-5,1e-4,1e-3,1e-2,1e-1,1,10]
lmbd_vals = [1e-5,1e-4,1e-3,1e-2,1e-1,1,10]

#make 3 lists for saving histogram values
list_eta_our=[]
list_lmbd_our=[]
list_r2_our=[]

for i, eta in enumerate(eta_vals):
    for j, lmbd in enumerate(lmbd_vals):
        np.random.seed(2204)
        hidden_weights1 = np.random.randn(n_features, n_hidden_neurons1)
        hidden_bias1 = np.zeros(n_hidden_neurons1) + 0.01
        np.random.seed(2204)
        hidden_weights2 = np.random.randn(n_hidden_neurons1, n_hidden_neurons2)
        hidden_bias2 = np.zeros(n_hidden_neurons2) + 0.01
        np.random.seed(2204)
        hidden_weights3 = np.random.randn(n_hidden_neurons2, n_hidden_neurons3)
        hidden_bias3 = np.zeros(n_hidden_neurons3) + 0.01
        np.random.seed(2204)
        output_weights = np.random.randn(n_hidden_neurons3, n_categories)
        output_bias = np.zeros(n_categories) + 0.01
        for i in range(100):
                # calculate gradients
            dWo, dBo, dWh1, dBh1, dWh2, dBh2, dWh3, dBh3 = backpropagation(XTrain, yTrain)

            # regularization term gradients
            dWo += lmbd * output_weights
            dWh1 += lmbd * hidden_weights1
            dWh2 += lmbd * hidden_weights2
            dWh3 += lmbd * hidden_weights3
         
            # update weights and biases
            output_weights -= eta * dWo
            output_bias -= eta * dBo
            hidden_weights1 -= eta * dWh1
            hidden_bias1 -= eta * dBh1
            hidden_weights2 -= eta * dWh2
            hidden_bias2 -= eta * dBh2
            hidden_weights3 -= eta * dWh3
            hidden_bias3 -= eta * dBh3
                
        test_predict = predict(XTest)
            
        print("Learning rate  = ", eta)
        print("Lambda = ", lmbd)
        print("R2 score on test set: ", r2_score(yTest, test_predict))
        print()
            
        list_eta_our.append(eta)
        list_lmbd_our.append(lmbd)
        list_r2_our.append(r2_score(yTest, test_predict))

np.random.seed(2204)
# weights and bias in the hidden layer
hidden_weights1 = np.random.randn(n_features, n_hidden_neurons1)
hidden_bias1 = np.zeros(n_hidden_neurons1) + 0.01
#print(hidden_bias1.shape)
np.random.seed(2204)
hidden_weights2 = np.random.randn(n_hidden_neurons1, n_hidden_neurons2)
hidden_bias2 = np.zeros(n_hidden_neurons2) + 0.01
np.random.seed(2204)
hidden_weights3 = np.random.randn(n_hidden_neurons2, n_hidden_neurons3)
hidden_bias3 = np.zeros(n_hidden_neurons3) + 0.01
np.random.seed(2204)
# weights and bias in the output layer
output_weights = np.random.randn(n_hidden_neurons3, n_categories)
output_bias = np.zeros(n_categories) + 0.01

eta = 1
lmbd = 0.00001
n_epochs=100
MSE_train = np.zeros(n_epochs)
R2 = np.zeros(n_epochs)
epochs = np.zeros(n_epochs)
for i in range(n_epochs):
    # calculate gradients
    dWo, dBo, dWh1, dBh1, dWh2, dBh2, dWh3, dBh3 = backpropagation(XTrain, yTrain)

    print(dWo)
    print(dWh1)
    print(dWh2)
    print(dWh3)

    # regularization term gradients
    dWo += lmbd * output_weights
    dWh1 += lmbd * hidden_weights1
    dWh2 += lmbd * hidden_weights2
    dWh3 += lmbd * hidden_weights3
 
    # update weights and biases
    output_weights -= eta * dWo
    output_bias -= eta * dBo
    hidden_weights1 -= eta * dWh1
    hidden_bias1 -= eta * dBh1
    hidden_weights2 -= eta * dWh2
    hidden_bias2 -= eta * dBh2
    hidden_weights3 -= eta * dWh3
    hidden_bias3 -= eta * dBh3
    
    print(output_weights)
    print(hidden_weights1)
    print(hidden_weights2)
    print(hidden_weights3)
    
    train_predict = predict(XTrain)
    test_predict = predict(XTest)
    MeanSquaredError_train= mean_squared_error(yTrain, train_predict)
    MSE_train[i] = MeanSquaredError_train
    R2[i] = r2_score(yTrain, train_predict)
    epochs[i]=i

test_predict = predict(XTest)
print("Learning rate  = ", eta)
print("Lambda = ", lmbd)
print("R2 score on test set: ", r2_score(yTest, test_predict))
print("MSE score on test set: ", mean_squared_error(yTest, test_predict))

    
fig1=plt.figure(figsize=(12,9))
plt.plot(epochs, MSE_train, label='Error')
plt.plot(epochs, R2, label='R2')
plt.ylabel(r'MSE')
plt.xlabel(r'Number of epochs')
plt.title(r'MSE vs epochs')
#plt.yscale('log')
plt.legend()
plt.show()
#plt.close()


# =============================================================================
# #print("New accuracy on training data: " + str(accuracy_score(predict(XTrain), yTrain)))
# test_predict = predict(XTest)
# print(test_predict)
# print(yTest.shape)
# #predicted = t_predict[:,0]
# y_values=yTest[:,0] 
# print(y_values.shape)
# 
# 
# print("Mean squared error: %.10f" % mean_squared_error(yTest, test_predict))
# print('Variance score: %.10f' % r2_score(yTest, test_predict))
# =============================================================================

# calculate edges of bins for plotting 2D histogram from used values lmbd_vals
def calc_edges(lmbd_vals):
    border=3
    edges=np.zeros(len(lmbd_vals)+1)
    edges[0]=border*lmbd_vals[0]/10
    for i in range(1,len(edges)):
        edges[i]=border*lmbd_vals[i-1]
    return edges

     
#plot histogram        
edges_lmbd = calc_edges(lmbd_vals)
edges_eta = calc_edges(eta_vals)

plt.rc('font', size=20)
fig1=plt.figure(figsize=(12,9))
fig1, ax1 = plt.subplots(figsize=(12,9))
h1=plt.hist2d(list_lmbd_our, list_eta_our, weights=list_r2_our, bins=(edges_lmbd,edges_eta))
ax1.set_aspect("equal")
hist, xbins, ybins, im = ax1.hist2d(list_lmbd_our, list_eta_our, weights=list_r2_our, bins=(edges_lmbd,edges_eta))
scale_hist=3 #position of numbers in histogram fields
for i in range(len(ybins)-1):
    for j in range(len(xbins)-1):
        ax1.text(scale_hist*xbins[j],scale_hist*ybins[i], round(hist[j,i],2), 
                color="w", ha="center", va="center", fontweight="bold", fontsize=15)
plt.colorbar(h1[3])
plt.ylabel(r'Learning rate')
plt.xlabel(r'Lambda')
plt.title(r'R2 of our code')
plt.xscale('log')
plt.yscale('log')
fig1.savefig('r2VsEtaVsLambda_our.png')
plt.show()
plt.close()   
    
from sklearn.neural_network import MLPRegressor

DNN_scikit = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)
#train_r2 = np.zeros((len(eta_vals), len(lmbd_vals)))
#sns.set()

eta_vals = [1e-5,1e-4,1e-3,1e-2,1e-1,1,10]
lmbd_vals = [1e-5,1e-4,1e-3,1e-2,1e-1,1,10]

#make 3 lists again
list_eta=[]
list_lmbd=[]
list_r2=[]

for i, eta in enumerate(eta_vals):
    for j, lmbd in enumerate(lmbd_vals):
        dnn = MLPRegressor(hidden_layer_sizes=(n_hidden_neurons1, n_hidden_neurons2, n_hidden_neurons3), alpha=lmbd, learning_rate_init=eta,
                           max_iter=100, activation='logistic', random_state=0)
        dnn.fit(XTrain, yTrain[:,0])
        DNN_scikit[i][j] = dnn
#       train_r2[i][j] = dnn.score(XTest, yTest[:,0])
        r2_nn_sl=dnn.score(XTest, yTest[:,0])
# =============================================================================
#       
        print("Learning rate  = ", eta)
        print("Lambda = ", lmbd)
        print("R2 on test set: ", dnn.score(XTest, yTest[:,0]))
        print()
        list_eta.append(eta)
        list_lmbd.append(lmbd)
        list_r2.append(r2_nn_sl)
# =============================================================================
fig21=plt.figure(figsize=(12,9))
fig21, ax21 = plt.subplots(figsize=(12,9))
h21=plt.hist2d(list_lmbd, list_eta, weights=list_r2, bins=(edges_lmbd,edges_eta))
ax21.set_aspect("equal")
hist2, xbins2, ybins2, im2 = ax21.hist2d(
list_lmbd, list_eta, weights=list_r2, bins=(edges_lmbd,edges_eta))
scale_hist=3 #position of numbers in histogram fields
for i in range(len(ybins2)-1):
    for j in range(len(xbins2)-1):
        ax21.text(scale_hist*xbins2[j],scale_hist*ybins2[i], round(hist2[j,i],4), 
                color="w", ha="center", va="center", fontweight="bold", fontsize=15)
plt.colorbar(h21[3])
plt.ylabel(r'Learning rate')
plt.xlabel(r'Lambda')
plt.title(r'R2 sklearn')
plt.xscale('log')
plt.yscale('log')
fig21.savefig('R2VsEtaVsLambda_skl_BC.png')
plt.show()
plt.close()

dnn = MLPRegressor(hidden_layer_sizes=(n_hidden_neurons1, n_hidden_neurons2, n_hidden_neurons3), alpha=0.0001, learning_rate_init=0.1,
                           max_iter=100, activation='logistic', random_state=0)
dnn.fit(XTrain, yTrain[:,0])
predictions=dnn.predict(XTest)
print("Learning rate  = ", 0.1)
print("Lambda = ", 0.0001)
print("R2 on test set: ", r2_score(yTest[:,0], predictions))
print("MSE on test set: ", mean_squared_error(yTest[:,0], predictions))
 

