#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Oct 25 18:08:45 2019

@author: Ary
"""


import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score


import seaborn as sns
import matplotlib.pyplot as plt


np.random.seed(2204)

## part a

def FrankeFunction(x,y):
    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))
    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))
    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))
    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)
    return term1 + term2 + term3 + term4

def Design_Matrix_X(x, y, n):
	N = len(x)
	l = int((n+1)*(n+2)/2)		
	X = np.ones((N,l))

	for i in range(1,n+1):
		q = int((i)*(i+1)/2)
		for k in range(i+1):
			X[:,q+k] = x**(i-k) * y**k

	return X

n_x=1000
m=5

x = np.random.uniform(0, 1, n_x)
y = np.random.uniform(0, 1, n_x)

z = FrankeFunction(x, y)

n = int(len(x))
z_1 = z
#z_1 = (z_1-np.mean(z_1))/np.std(z_1)
#z_1, z_1=np.meshgrid(z_1, z_1)
#array = array[:,np.newaxis]
z_1 = z_1[:,np.newaxis]
#z_1=z_1.reshape(1000,1)
print(z_1.shape)
X= Design_Matrix_X(x,y,n=m)
DesignMatrix = pd.DataFrame(X)
print(DesignMatrix.shape)

trainingShare = 0.8
seed  = 1
XTrain, XTest, yTrain, yTest=train_test_split(X, z_1, train_size=trainingShare, \
                                              test_size = 1-trainingShare,
                                             random_state=seed)

print(yTrain.shape)

n_inputs, n_features = XTrain.shape
#come scelgo il numero di categorie
n_categories = 1
n_hidden_neurons1 = 50
n_hidden_neurons2 = 40
n_hidden_neurons3 = 30

def relu(X):
   return np.maximum(0,X)

def sigmoid(x):
    return 1/(1 + np.exp(-x))


# weights and bias in the hidden layer
hidden_weights1 = np.random.randn(n_features, n_hidden_neurons1)
hidden_bias1 = np.zeros(n_hidden_neurons1) + 0.01
#print(hidden_bias1.shape)

hidden_weights2 = np.random.randn(n_hidden_neurons1, n_hidden_neurons2)
hidden_bias2 = np.zeros(n_hidden_neurons2) + 0.01

hidden_weights3 = np.random.randn(n_hidden_neurons2, n_hidden_neurons3)
hidden_bias3 = np.zeros(n_hidden_neurons3) + 0.01

# weights and bias in the output layer
output_weights = np.random.randn(n_hidden_neurons3, n_categories)
output_bias = np.zeros(n_categories) + 0.01

print(hidden_weights1)
print(hidden_weights2)
print(hidden_weights3)

def feed_forward(X):
    # weighted sum of inputs to the hidden layer
    z_h1 = np.matmul(X, hidden_weights1) + hidden_bias1
    #print(z_h1.shape)
    # activation in the hidden layer
    #a_h1 = relu(z_h1)
    a_h1 = sigmoid(z_h1)
    #print(a_h1.shape)
    
    z_h2 = np.matmul(a_h1, hidden_weights2) + hidden_bias2
    #print(z_h2)
    # activation in the hidden layer
    #a_h2 = relu(z_h2)
    a_h2 = sigmoid(z_h2)
   # print(a_h2)
    
    z_h3 = np.matmul(a_h2, hidden_weights3) + hidden_bias3
    #print(z_h3)
    # activation in the hidden layer
    #a_h3 = relu(z_h3)
    a_h3 = sigmoid(z_h3)
    #sprint(a_h3)
    
    # weighted sum of inputs to the output layer
    z_o = np.matmul(a_h3, output_weights) + output_bias
    a_ho = sigmoid(z_o)
# =============================================================================
#     exp_term = np.exp(z_o)
#     outputs = exp_term / np.sum(exp_term, axis=1, keepdims=True)
# =============================================================================
    #print(z_o) 
    
    outputs = a_ho #is it the right function for output values?

    return outputs


def predict(X):
    outputs = feed_forward(X)
# =============================================================================
#     print(probabilities)
   # a=np.argmax(outputs, axis=1)
# =============================================================================
    #out = (np.sum(outputs, axis=1, keepdims=True))/200
    return outputs

def feed_forward_train(X):

    z_h1 = np.matmul(X, hidden_weights1) + hidden_bias1
    #print(z_h1.shape)
    # activation in the hidden layer
    #a_h1 = relu(z_h1)
    a_h1 = sigmoid(z_h1)
    #print(a_h1.shape)
    
    z_h2 = np.matmul(a_h1, hidden_weights2) + hidden_bias2
    #print(z_h2.shape)
    # activation in the hidden layer
    #a_h2 = relu(z_h2)
    a_h2 = sigmoid(z_h2)
    #print(a_h2.shape)
    
    z_h3 = np.matmul(a_h2, hidden_weights3) + hidden_bias3
    #print(z_h3.shape)
    # activation in the hidden layer
    #a_h3 = relu(z_h3)
    a_h3 = sigmoid(z_h3)
    
    # weighted sum of inputs to the output layer
    z_o = np.matmul(a_h3, output_weights) + output_bias
    a_ho = sigmoid(z_o)
    # axis 0 holds each input and axis 1 the probabilities of each category
    #print(z_o.shape)
    outputs = a_ho
    return a_h1, a_h2, a_h3, a_ho, outputs

def backpropagation(X, Y):
    a_h1, a_h2, a_h3, a_ho, outputs = feed_forward_train(X)

    n = Y.shape[0]
    term = outputs - Y
 #   print(term.shape)
    error_output = ((2/n)*term) * a_ho * (1 - a_ho)
    
# =============================================================================
#     print(error_output.shape)
#     print(output_weights.T.shape)
#     print(np.matmul(error_output, output_weights.T).shape)
#     print(a_h3.shape)
#   #  print(a_h3.shape)
# =============================================================================
    
    error_hidden3 = np.matmul(error_output, output_weights.T) * a_h3 * (1 - a_h3)
    error_hidden2 = np.matmul(error_hidden3, hidden_weights3.T) * a_h2 * (1 - a_h2)
    error_hidden1 = np.matmul(error_hidden2, hidden_weights2.T) * a_h1 * (1 - a_h1)

    output_weights_gradient = np.matmul(a_h3.T, error_output)
    output_bias_gradient = np.sum(error_output, axis=0)
    
    hidden_weights_gradient3 = np.matmul(a_h2.T, error_hidden3)
    hidden_bias_gradient3 = np.sum(error_hidden3, axis=0)
    
    hidden_weights_gradient2 = np.matmul(a_h1.T, error_hidden2)
    hidden_bias_gradient2 = np.sum(error_hidden2, axis=0)
    
    hidden_weights_gradient1 = np.matmul(X.T, error_hidden1)
    hidden_bias_gradient1 = np.sum(error_hidden1, axis=0)
    
    return output_weights_gradient, output_bias_gradient, hidden_weights_gradient1, hidden_bias_gradient1,hidden_weights_gradient2, hidden_bias_gradient2,hidden_weights_gradient3, hidden_bias_gradient3


eta = 1
lmbd = 0.001
n_epochs=100
MSE = np.zeros(n_epochs)
R2 = np.zeros(n_epochs)
epochs = np.zeros(n_epochs)
for i in range(n_epochs):
    # calculate gradients
    dWo, dBo, dWh1, dBh1, dWh2, dBh2, dWh3, dBh3 = backpropagation(XTrain, yTrain)

    print(dWo)
    print(dWh1)
    print(dWh2)
    print(dWh3)

    # regularization term gradients
    dWo += lmbd * output_weights
    dWh1 += lmbd * hidden_weights1
    dWh2 += lmbd * hidden_weights2
    dWh3 += lmbd * hidden_weights3
 
    # update weights and biases
    output_weights -= eta * dWo
    output_bias -= eta * dBo
    hidden_weights1 -= eta * dWh1
    hidden_bias1 -= eta * dBh1
    hidden_weights2 -= eta * dWh2
    hidden_bias2 -= eta * dBh2
    hidden_weights3 -= eta * dWh3
    hidden_bias3 -= eta * dBh3
    
    print(output_weights)
    print(hidden_weights1)
    print(hidden_weights2)
    print(hidden_weights3)
    
    train_predict = predict(XTrain)
    MeanSquaredError= mean_squared_error(yTrain, train_predict)
    MSE[i] = MeanSquaredError
    R2[i] = r2_score(yTrain, train_predict)
    epochs[i]=i
    
fig1=plt.figure(figsize=(12,9))
plt.plot(epochs, MSE, label='Error')
plt.plot(epochs, R2, label='R2')
plt.ylabel(r'MSE')
plt.xlabel(r'Number of epochs')
plt.title(r'MSE vs epochs')
#plt.yscale('log')
plt.legend()
plt.show()
#plt.close()

#print("New accuracy on training data: " + str(accuracy_score(predict(XTrain), yTrain)))
test_predict = predict(XTest)
print(test_predict)
print(yTest.shape)
#predicted = t_predict[:,0]
y_values=yTest[:,0] 
print(y_values.shape)


print("Mean squared error: %.10f" % mean_squared_error(yTest, test_predict))
print('Variance score: %.10f' % r2_score(yTest, test_predict))

eta_vals = np.logspace(-5, 1, 7)
lmbd_vals = np.logspace(-5, 1, 7)

#make 3 lists for saving histogram values
list_eta_our=[]
list_lmbd_our=[]
list_r2_our=[]

for i, eta in enumerate(eta_vals):
    for j, lmbd in enumerate(lmbd_vals):
        hidden_weights1 = np.random.randn(n_features, n_hidden_neurons1)
        hidden_bias1 = np.zeros(n_hidden_neurons1) + 0.01

        hidden_weights2 = np.random.randn(n_hidden_neurons1, n_hidden_neurons2)
        hidden_bias2 = np.zeros(n_hidden_neurons2) + 0.01

        hidden_weights3 = np.random.randn(n_hidden_neurons2, n_hidden_neurons3)
        hidden_bias3 = np.zeros(n_hidden_neurons3) + 0.01

# weights and bias in the output layer
        output_weights = np.random.randn(n_hidden_neurons3, n_categories)
        output_bias = np.zeros(n_categories) + 0.01
        for i in range(100):
            # calculate gradients
            dWo, dBo, dWh1, dBh1, dWh2, dBh2, dWh3, dBh3 = backpropagation(XTrain, yTrain)

            # regularization term gradients
            dWo += lmbd * output_weights
            dWh1 += lmbd * hidden_weights1
            dWh2 += lmbd * hidden_weights2
            dWh3 += lmbd * hidden_weights3
         
            # update weights and biases
            output_weights -= eta * dWo
            output_bias -= eta * dBo
            hidden_weights1 -= eta * dWh1
            hidden_bias1 -= eta * dBh1
            hidden_weights2 -= eta * dWh2
            hidden_bias2 -= eta * dBh2
            hidden_weights3 -= eta * dWh3
            hidden_bias3 -= eta * dBh3
            
        test_predict = predict(XTest)
        
        print("Learning rate  = ", eta)
        print("Lambda = ", lmbd)
        print("R2 score on test set: ", r2_score(yTest, test_predict))
        print()
        
        list_eta_our.append(eta)
        list_lmbd_our.append(lmbd)
        list_r2_our.append(r2_score(yTest, test_predict))
   
# calculate edges of bins for plotting 2D histogram from used values lmbd_vals
def calc_edges(lmbd_vals):
    border=3
    edges=np.zeros(len(lmbd_vals)+1)
    edges[0]=border*lmbd_vals[0]/10
    for i in range(1,len(edges)):
        edges[i]=border*lmbd_vals[i-1]
    return edges

     
#plot histogram        
edges_lmbd = calc_edges(lmbd_vals)
edges_eta = calc_edges(eta_vals)

plt.rc('font', size=20)
fig1=plt.figure(figsize=(12,9))
fig1, ax1 = plt.subplots(figsize=(12,9))
h1=plt.hist2d(list_lmbd_our, list_eta_our, weights=list_r2_our, bins=(edges_lmbd,edges_eta))
ax1.set_aspect("equal")
hist, xbins, ybins, im = ax1.hist2d(list_lmbd_our, list_eta_our, weights=list_r2_our, bins=(edges_lmbd,edges_eta))
scale_hist=3 #position of numbers in histogram fields
for i in range(len(ybins)-1):
    for j in range(len(xbins)-1):
        ax1.text(scale_hist*xbins[j],scale_hist*ybins[i], round(hist[j,i],2), 
                color="w", ha="center", va="center", fontweight="bold", fontsize=15)
plt.colorbar(h1[3])
plt.ylabel(r'Learning rate')
plt.xlabel(r'Lambda')
plt.title(r'Accuracy of our code')
plt.xscale('log')
plt.yscale('log')
fig1.savefig('AccuracyVsEtaVsLambda_our.png')
plt.show()
plt.close()   
    


eta_vals = np.logspace(-5, 1, 7)
lmbd_vals = np.logspace(-5, 1, 7)
print(yTrain.shape)
print(yTest.shape)

from sklearn.neural_network import MLPRegressor

DNN_scikit = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)
train_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))
sns.set()

for i, eta in enumerate(eta_vals):
    for j, lmbd in enumerate(lmbd_vals):
        dnn = MLPRegressor(hidden_layer_sizes=(n_hidden_neurons1, n_hidden_neurons2, n_hidden_neurons3),alpha=lmbd, learning_rate_init=eta, max_iter=100, activation='logistic')
        dnn.fit(XTrain, yTrain[:,0])
        DNN_scikit[i][j] = dnn
        train_accuracy[i][j] = dnn.score(XTest, yTest[:,0])
        
# =============================================================================
#         
        print("Learning rate  = ", eta)
        print("Lambda = ", lmbd)
        print("R2 on test set: ", dnn.score(XTest, yTest[:,0]))
        print()
# =============================================================================
fig, ax = plt.subplots(figsize = (10, 10))
sns.heatmap(train_accuracy, annot=True, ax=ax, cmap="viridis")
ax.set_title("Training Accuracy")
ax.set_ylabel("$\eta$")
ax.set_xlabel("$\lambda$")
plt.show()   
